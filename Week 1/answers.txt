Q1) No. The WordCount code works by taking in the string input and tokenizing them into words, breaking them on spaces. The mappers then append a single number or value 1 to each word and produces the outputs as key value pairs such as (word, 1). After this, the shuffling and sorting takes place implicitly which collects all the key value pairs in with a specific key and these are then passed onto the reducers. The input to the reducer is also defined as key value pairs which then sums the values of each pair thus producing output key value pairs such as (word, totalCount) thus giving the word count of every word. Q2) The output produced when the number of reducers (by default is 1) is changed to 3 is different as in the case of reducers set to 1, the output file is in a single part whereas when we increase the number of reducers, three output files are produced. This is because the number of partitions that take place post mapping depends on the number of reducers. Thus the three reducers work on three separate segments of the data, sum up their values and produce three different output files. In this case, the shuffle maps are tripled as well in comparison to a single reducer. However, due to the file size not being too large, changing the number of reducers does not imply a very large magnitude of change when it comes to shuffling of data before fed into the reducer. However, in cases of very large datasets, increasing the number of reducers could lead to a significant increase in shuffle bytes. Further, increasing the number of reducers could also have benefits in case of large datasets since their parallel execution implies a more efficient use of (available) cluster resources and provides better load balancing overall. Since the outputs would be segmented and hence reduced in size upon adding more reducers, it would be more beneficial for big datasets and also allows for better inspection of data outputs.Q3) When the number of reducers are set to 0, no reduce phase is initiated and thus the output from the mapper is considered the final output. In this case as well, 3 output files are produced as all of the key-value pairs found during the mapping are the output and are thus segmented into the configured or default size. We can also observe that the total number of bytes written are much higher (as understood from the o/p in the terminal) since only mapping phase and no reduction has taken place and thus all the key-value pairs found through the mapping phase are the output.Q4) The combiner plays a vital role in reducing the number of shuffle outputs which is noticeable in the terminal outputs since the reduce shuffle bytes decrease significantly. This is because the combiner in this case already adds up the values of the associated keys thus requiring much less shuffling before the reducer phase (which calculates the averages as well). The number of input records to the reducer is also hence significantly lesser in the case where the combiner is used. Due to all of these, the running time is also slightly reduced with the combiner optimization. 